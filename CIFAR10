REFERENCES = [Python for Computer Vision with OpenCV and Deep Learning](https://www.udemy.com/course/python-for-computer-vision-with-opencv-and-deep-learning/learn/lecture/12257908#questions)

# KERAS CNN WITH CIFAR-10 colur images for varios objects


import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense,Conv2D,MaxPool2D,Flatten
from sklearn.metrics import classification_report

(x_train,y_train) , (x_test, y_test) = cifar10.load_data()
x_train.shape #((50000, 32, 32, 3))
y_train.shape #(50000, 1)

single = x_train[0]
single
plt.imshow(single, cmap = 'gray') 
plt.imshow(single, cmap = 'gray_r')            # 0's dark and 1 's white pixel in grayscale mode, inverse of grayscale

y_train                                        #convert the labelling to one hot encoding/ change to the distinct categories of 0 and 1
y_cat_test = to_categorical(y_test,10)         # 10 because 0 to 9 we have digits so 10 categories/ categorical datas.
y_cat_train = to_categorical(y_train,10)
y_cat_test
                                                
x_train = x_train/x_train.max()                          #processing X DATAS by normalization or lots of different ways, here values will be between 0 and 1 after the normalisation.
x_test = x_test/x_train.max() 

x_train
x_test.shape




model = Sequential()
#Convolutional Layer
model.add(Conv2D(filters =50, kernel_size = (4,4), input_shape = (32,32,3),activation = 'relu'))
#Pooling Layer.
model.add(MaxPool2D(pool_size =(5,5)))
#2D to 1D
model.add(Flatten())

model.add(Dense(300, activation = 'relu'))


model.add(Dense(10, activation = 'softmax'))  #softmax for multiclass classification


model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop',metrics = ['accuracy'])

model.summary()

model.fit(x_train,y_cat_train,epochs =20)


# model.evaluate(x_test,y_cat_test)   #overfitting, if performs good on training and not good on test it is overfitting, and underfitting is the vice versa.
#                                     #the accuracy of mostly both of train and test set is simillar so it may be here not a problem
                                    

y_cat_test_arg= np.argmax(y_cat_test,axis=1)
predictions = np.argmax(model.predict(x_test), axis =1) #conversion of one hot encoding to values. np.argmax returns the indices of the index no. axis = 1 direction along columns
print(classification_report(y_cat_test_arg,predictions))
y_cat_test_arg
predictions
